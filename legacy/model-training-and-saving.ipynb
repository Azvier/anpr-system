{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "from sklearn.metrics import f1_score \n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the root directory containing the class folders\n",
    "root_path = \"Train Data/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1\n",
    "Simple model with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1953 images belonging to 36 classes.\n",
      "Found 470 images belonging to 36 classes.\n",
      "Epoch 1/100\n",
      "31/31 [==============================] - 4s 111ms/step - loss: 2.2710 - accuracy: 0.4419 - val_loss: 1.2636 - val_accuracy: 0.7106\n",
      "Epoch 2/100\n",
      "31/31 [==============================] - 3s 105ms/step - loss: 0.9656 - accuracy: 0.7486 - val_loss: 0.6065 - val_accuracy: 0.8447\n",
      "Epoch 3/100\n",
      "31/31 [==============================] - 3s 103ms/step - loss: 0.5122 - accuracy: 0.8669 - val_loss: 0.4811 - val_accuracy: 0.8638\n",
      "Epoch 4/100\n",
      "31/31 [==============================] - 3s 103ms/step - loss: 0.2991 - accuracy: 0.9130 - val_loss: 0.3501 - val_accuracy: 0.8979\n",
      "Epoch 5/100\n",
      "31/31 [==============================] - 3s 107ms/step - loss: 0.1917 - accuracy: 0.9447 - val_loss: 0.3584 - val_accuracy: 0.8957\n",
      "Epoch 6/100\n",
      "31/31 [==============================] - 3s 105ms/step - loss: 0.1541 - accuracy: 0.9555 - val_loss: 0.3157 - val_accuracy: 0.9128\n",
      "Epoch 7/100\n",
      "31/31 [==============================] - 3s 108ms/step - loss: 0.1065 - accuracy: 0.9724 - val_loss: 0.3262 - val_accuracy: 0.9106\n",
      "Epoch 8/100\n",
      "31/31 [==============================] - 3s 104ms/step - loss: 0.0940 - accuracy: 0.9703 - val_loss: 0.3016 - val_accuracy: 0.9149\n",
      "Epoch 9/100\n",
      "31/31 [==============================] - 3s 112ms/step - loss: 0.0938 - accuracy: 0.9683 - val_loss: 0.3013 - val_accuracy: 0.9106\n",
      "Epoch 10/100\n",
      "31/31 [==============================] - 4s 120ms/step - loss: 0.0498 - accuracy: 0.9816 - val_loss: 0.3133 - val_accuracy: 0.9234\n",
      "Epoch 11/100\n",
      "31/31 [==============================] - 3s 108ms/step - loss: 0.0376 - accuracy: 0.9892 - val_loss: 0.3048 - val_accuracy: 0.9255\n",
      "Epoch 12/100\n",
      "31/31 [==============================] - 3s 109ms/step - loss: 0.0436 - accuracy: 0.9877 - val_loss: 0.3137 - val_accuracy: 0.9234\n",
      "Epoch 13/100\n",
      "31/31 [==============================] - 3s 112ms/step - loss: 0.0542 - accuracy: 0.9846 - val_loss: 0.3016 - val_accuracy: 0.9234\n",
      "Epoch 14/100\n",
      "31/31 [==============================] - 3s 110ms/step - loss: 0.0401 - accuracy: 0.9877 - val_loss: 0.3452 - val_accuracy: 0.9170\n",
      "Epoch 15/100\n",
      "31/31 [==============================] - 4s 117ms/step - loss: 0.0354 - accuracy: 0.9892 - val_loss: 0.2920 - val_accuracy: 0.9255\n",
      "Epoch 16/100\n",
      "31/31 [==============================] - 4s 113ms/step - loss: 0.0331 - accuracy: 0.9913 - val_loss: 0.3298 - val_accuracy: 0.9213\n",
      "Epoch 17/100\n",
      "31/31 [==============================] - 3s 109ms/step - loss: 0.0299 - accuracy: 0.9913 - val_loss: 0.3334 - val_accuracy: 0.9213\n",
      "Epoch 18/100\n",
      "31/31 [==============================] - 3s 111ms/step - loss: 0.0177 - accuracy: 0.9944 - val_loss: 0.4001 - val_accuracy: 0.9191\n",
      "Epoch 19/100\n",
      "31/31 [==============================] - 3s 106ms/step - loss: 0.0276 - accuracy: 0.9933 - val_loss: 0.3638 - val_accuracy: 0.9255\n",
      "Epoch 20/100\n",
      "31/31 [==============================] - 3s 111ms/step - loss: 0.0260 - accuracy: 0.9908 - val_loss: 0.4085 - val_accuracy: 0.9191\n",
      "Epoch 21/100\n",
      "31/31 [==============================] - 3s 106ms/step - loss: 0.0275 - accuracy: 0.9913 - val_loss: 0.3799 - val_accuracy: 0.9191\n",
      "Epoch 22/100\n",
      "31/31 [==============================] - 3s 105ms/step - loss: 0.0283 - accuracy: 0.9898 - val_loss: 0.4427 - val_accuracy: 0.9234\n",
      "Epoch 23/100\n",
      "31/31 [==============================] - 3s 110ms/step - loss: 0.0225 - accuracy: 0.9944 - val_loss: 0.3843 - val_accuracy: 0.9234\n",
      "Epoch 24/100\n",
      "31/31 [==============================] - 4s 115ms/step - loss: 0.0221 - accuracy: 0.9939 - val_loss: 0.3454 - val_accuracy: 0.9234\n",
      "Epoch 25/100\n",
      "31/31 [==============================] - 3s 109ms/step - loss: 0.0143 - accuracy: 0.9954 - val_loss: 0.3486 - val_accuracy: 0.9362\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(validation_split=0.2, rescale=1./255)  # use 20% of data for validation\n",
    "\n",
    "# Load images from directories\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    root_path,\n",
    "    target_size=(40, 60),  # you may need to adjust the target size depending on your image dimensions\n",
    "    color_mode='grayscale',\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    root_path,\n",
    "    target_size=(40, 60),  # you may need to adjust the target size depending on your image dimensions\n",
    "    color_mode='grayscale',\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "num_classes = len(train_generator.class_indices)  # get the number of classes\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(40, 60, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))  # the output layer should have one neuron per class\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', # which metric to monitor\n",
    "    min_delta=0.001, # minimum change in the monitored metric to be considered as an improvement\n",
    "    patience=10, # number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True, # whether to restore model weights from the epoch with the best value of the monitored metric\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator, \n",
    "    epochs=100, \n",
    "    validation_data=val_generator, \n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "model.save(r'model jadi/model 1.h5')  # saves the model weights and architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2 - Simpler model than model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1953 images belonging to 36 classes.\n",
      "Found 470 images belonging to 36 classes.\n",
      "Epoch 1/100\n",
      "31/31 [==============================] - 2s 42ms/step - loss: 2.2852 - accuracy: 0.4322 - val_loss: 1.2618 - val_accuracy: 0.7021\n",
      "Epoch 2/100\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 1.1738 - accuracy: 0.6836 - val_loss: 0.7825 - val_accuracy: 0.7851\n",
      "Epoch 3/100\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.7907 - accuracy: 0.7793 - val_loss: 0.6128 - val_accuracy: 0.8426\n",
      "Epoch 4/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.6349 - accuracy: 0.8315 - val_loss: 0.5165 - val_accuracy: 0.8660\n",
      "Epoch 5/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.4745 - accuracy: 0.8618 - val_loss: 0.4310 - val_accuracy: 0.8915\n",
      "Epoch 6/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.4066 - accuracy: 0.8935 - val_loss: 0.3906 - val_accuracy: 0.8851\n",
      "Epoch 7/100\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 0.3110 - accuracy: 0.9140 - val_loss: 0.3834 - val_accuracy: 0.8894\n",
      "Epoch 8/100\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 0.2575 - accuracy: 0.9283 - val_loss: 0.3804 - val_accuracy: 0.8979\n",
      "Epoch 9/100\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.2145 - accuracy: 0.9386 - val_loss: 0.3479 - val_accuracy: 0.9021\n",
      "Epoch 10/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.1767 - accuracy: 0.9473 - val_loss: 0.3765 - val_accuracy: 0.8957\n",
      "Epoch 11/100\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 0.1367 - accuracy: 0.9611 - val_loss: 0.3945 - val_accuracy: 0.8915\n",
      "Epoch 12/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.1546 - accuracy: 0.9529 - val_loss: 0.3641 - val_accuracy: 0.8979\n",
      "Epoch 13/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.1166 - accuracy: 0.9657 - val_loss: 0.3772 - val_accuracy: 0.8894\n",
      "Epoch 14/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.1095 - accuracy: 0.9713 - val_loss: 0.3484 - val_accuracy: 0.9085\n",
      "Epoch 15/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.0938 - accuracy: 0.9734 - val_loss: 0.3602 - val_accuracy: 0.9021\n",
      "Epoch 16/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.0739 - accuracy: 0.9790 - val_loss: 0.3689 - val_accuracy: 0.9021\n",
      "Epoch 17/100\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 0.0620 - accuracy: 0.9841 - val_loss: 0.3771 - val_accuracy: 0.9085\n",
      "Epoch 18/100\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 0.0635 - accuracy: 0.9836 - val_loss: 0.3821 - val_accuracy: 0.9000\n",
      "Epoch 19/100\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 0.0631 - accuracy: 0.9805 - val_loss: 0.3657 - val_accuracy: 0.9128\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(validation_split=0.2, rescale=1./255)  # use 20% of data for validation\n",
    "\n",
    "# Load images from directories\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    root_path,\n",
    "    target_size=(40, 60),  # you may need to adjust the target size depending on your image dimensions\n",
    "    color_mode='grayscale',\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    root_path,\n",
    "    target_size=(40, 60),  # you may need to adjust the target size depending on your image dimensions\n",
    "    color_mode='grayscale',\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "num_classes = len(train_generator.class_indices)  # get the number of classes\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=(40, 60, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', # which metric to monitor\n",
    "    min_delta=0.001, # minimum change in the monitored metric to be considered as an improvement\n",
    "    patience=10, # number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True, # whether to restore model weights from the epoch with the best value of the monitored metric\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator, \n",
    "    epochs=100, \n",
    "    validation_data=val_generator, \n",
    "    callbacks=[early_stopping] #, reduce_lr\n",
    ")\n",
    "model.save(r'model jadi/model 2.h5')  # saves the model weights and architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3 - Model 2 but batch size 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1953 images belonging to 36 classes.\n",
      "Found 470 images belonging to 36 classes.\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 2s 72ms/step - loss: 2.7758 - accuracy: 0.3236 - val_loss: 1.8991 - val_accuracy: 0.5872 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 1.7590 - accuracy: 0.5684 - val_loss: 1.2436 - val_accuracy: 0.6936 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 1.2664 - accuracy: 0.6692 - val_loss: 0.9250 - val_accuracy: 0.7574 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.9662 - accuracy: 0.7481 - val_loss: 0.6995 - val_accuracy: 0.8255 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.7842 - accuracy: 0.7849 - val_loss: 0.5928 - val_accuracy: 0.8532 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.6857 - accuracy: 0.8105 - val_loss: 0.5533 - val_accuracy: 0.8468 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.5861 - accuracy: 0.8413 - val_loss: 0.4953 - val_accuracy: 0.8851 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.5210 - accuracy: 0.8587 - val_loss: 0.4549 - val_accuracy: 0.8957 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.4469 - accuracy: 0.8802 - val_loss: 0.4387 - val_accuracy: 0.8936 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.3909 - accuracy: 0.8920 - val_loss: 0.4375 - val_accuracy: 0.8851 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.3515 - accuracy: 0.8981 - val_loss: 0.4129 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.3259 - accuracy: 0.9109 - val_loss: 0.4037 - val_accuracy: 0.9000 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.2962 - accuracy: 0.9176 - val_loss: 0.4009 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.2765 - accuracy: 0.9227 - val_loss: 0.3732 - val_accuracy: 0.9000 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.2595 - accuracy: 0.9278 - val_loss: 0.3729 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.2270 - accuracy: 0.9386 - val_loss: 0.3990 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.2169 - accuracy: 0.9365 - val_loss: 0.3654 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.1774 - accuracy: 0.9519 - val_loss: 0.3610 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.1639 - accuracy: 0.9534 - val_loss: 0.3661 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.1662 - accuracy: 0.9508 - val_loss: 0.3653 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.1478 - accuracy: 0.9652 - val_loss: 0.3535 - val_accuracy: 0.9149 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.1332 - accuracy: 0.9626 - val_loss: 0.3751 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.1266 - accuracy: 0.9642 - val_loss: 0.3448 - val_accuracy: 0.9149 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.1353 - accuracy: 0.9585 - val_loss: 0.3516 - val_accuracy: 0.9149 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.1093 - accuracy: 0.9724 - val_loss: 0.3850 - val_accuracy: 0.9000 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.1045 - accuracy: 0.9698 - val_loss: 0.3742 - val_accuracy: 0.9106 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.1121 - accuracy: 0.9688 - val_loss: 0.3428 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.1013 - accuracy: 0.9693 - val_loss: 0.3500 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0982 - accuracy: 0.9718 - val_loss: 0.3598 - val_accuracy: 0.9064 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0892 - accuracy: 0.9734 - val_loss: 0.3718 - val_accuracy: 0.9149 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.0777 - accuracy: 0.9790 - val_loss: 0.3490 - val_accuracy: 0.9106 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.0818 - accuracy: 0.9744 - val_loss: 0.4147 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.0766 - accuracy: 0.9775 - val_loss: 0.3569 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 0.0746 - accuracy: 0.9831 - val_loss: 0.3797 - val_accuracy: 0.9064 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 0.0775 - accuracy: 0.9775 - val_loss: 0.3876 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0674 - accuracy: 0.9816 - val_loss: 0.3800 - val_accuracy: 0.9149 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.0485 - accuracy: 0.9892 - val_loss: 0.3579 - val_accuracy: 0.9213 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(validation_split=0.2, rescale=1./255)  # use 20% of data for validation\n",
    "\n",
    "# Load images from directories\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    root_path,\n",
    "    target_size=(40, 60),  # you may need to adjust the target size depending on your image dimensions\n",
    "    color_mode='grayscale',\n",
    "    batch_size=128,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    root_path,\n",
    "    target_size=(40, 60),  # you may need to adjust the target size depending on your image dimensions\n",
    "    color_mode='grayscale',\n",
    "    batch_size=128,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "num_classes = len(train_generator.class_indices)  # get the number of classes\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=(40, 60, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', # which metric to monitor\n",
    "    min_delta=0.001, # minimum change in the monitored metric to be considered as an improvement\n",
    "    patience=10, # number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True, # whether to restore model weights from the epoch with the best value of the monitored metric\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator, \n",
    "    epochs=100, \n",
    "    validation_data=val_generator, \n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "model.save(r'model jadi/model 3.h5')  # saves the model weights and architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 4 - Model 2 but 256 batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1953 images belonging to 36 classes.\n",
      "Found 470 images belonging to 36 classes.\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 2s 178ms/step - loss: 3.0048 - accuracy: 0.2509 - val_loss: 2.2294 - val_accuracy: 0.4745 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 1s 150ms/step - loss: 2.1257 - accuracy: 0.4818 - val_loss: 1.6359 - val_accuracy: 0.6489 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 1s 147ms/step - loss: 1.6216 - accuracy: 0.6032 - val_loss: 1.2232 - val_accuracy: 0.7021 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 1s 158ms/step - loss: 1.2867 - accuracy: 0.6723 - val_loss: 0.9882 - val_accuracy: 0.7574 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 1s 145ms/step - loss: 1.0828 - accuracy: 0.7245 - val_loss: 0.8389 - val_accuracy: 0.7872 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 1s 141ms/step - loss: 0.9479 - accuracy: 0.7512 - val_loss: 0.7350 - val_accuracy: 0.8149 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.8241 - accuracy: 0.7788 - val_loss: 0.6650 - val_accuracy: 0.8213 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 1s 163ms/step - loss: 0.7263 - accuracy: 0.7957 - val_loss: 0.6120 - val_accuracy: 0.8447 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 1s 163ms/step - loss: 0.6884 - accuracy: 0.8044 - val_loss: 0.5745 - val_accuracy: 0.8489 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 1s 160ms/step - loss: 0.6247 - accuracy: 0.8264 - val_loss: 0.5245 - val_accuracy: 0.8660 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 1s 159ms/step - loss: 0.5648 - accuracy: 0.8336 - val_loss: 0.4964 - val_accuracy: 0.8532 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 1s 176ms/step - loss: 0.5205 - accuracy: 0.8479 - val_loss: 0.4774 - val_accuracy: 0.8702 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 1s 164ms/step - loss: 0.4986 - accuracy: 0.8561 - val_loss: 0.4808 - val_accuracy: 0.8553 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 1s 173ms/step - loss: 0.4761 - accuracy: 0.8566 - val_loss: 0.4624 - val_accuracy: 0.8766 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 1s 161ms/step - loss: 0.4154 - accuracy: 0.8735 - val_loss: 0.4462 - val_accuracy: 0.8745 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 1s 165ms/step - loss: 0.4185 - accuracy: 0.8822 - val_loss: 0.4170 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 1s 168ms/step - loss: 0.3830 - accuracy: 0.8920 - val_loss: 0.4022 - val_accuracy: 0.8851 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 1s 160ms/step - loss: 0.3431 - accuracy: 0.9104 - val_loss: 0.3970 - val_accuracy: 0.8851 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 1s 162ms/step - loss: 0.3321 - accuracy: 0.9089 - val_loss: 0.3946 - val_accuracy: 0.8830 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 1s 154ms/step - loss: 0.3128 - accuracy: 0.9160 - val_loss: 0.3999 - val_accuracy: 0.8851 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 1s 151ms/step - loss: 0.3018 - accuracy: 0.9135 - val_loss: 0.3983 - val_accuracy: 0.8809 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 1s 164ms/step - loss: 0.2925 - accuracy: 0.9119 - val_loss: 0.3764 - val_accuracy: 0.8809 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 1s 161ms/step - loss: 0.2642 - accuracy: 0.9273 - val_loss: 0.3897 - val_accuracy: 0.8851 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 1s 155ms/step - loss: 0.2586 - accuracy: 0.9268 - val_loss: 0.3919 - val_accuracy: 0.8766 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 1s 161ms/step - loss: 0.2451 - accuracy: 0.9273 - val_loss: 0.3802 - val_accuracy: 0.8872 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 1s 151ms/step - loss: 0.2194 - accuracy: 0.9345 - val_loss: 0.3805 - val_accuracy: 0.8745 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 1s 150ms/step - loss: 0.2282 - accuracy: 0.9339 - val_loss: 0.3676 - val_accuracy: 0.8894 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 1s 159ms/step - loss: 0.2256 - accuracy: 0.9324 - val_loss: 0.3663 - val_accuracy: 0.8872 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.1935 - accuracy: 0.9457 - val_loss: 0.3733 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 1s 151ms/step - loss: 0.1929 - accuracy: 0.9427 - val_loss: 0.3618 - val_accuracy: 0.9000 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 1s 166ms/step - loss: 0.2026 - accuracy: 0.9432 - val_loss: 0.3636 - val_accuracy: 0.9000 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 1s 158ms/step - loss: 0.1884 - accuracy: 0.9467 - val_loss: 0.3800 - val_accuracy: 0.8936 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 1s 174ms/step - loss: 0.1900 - accuracy: 0.9427 - val_loss: 0.3523 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 1s 166ms/step - loss: 0.1839 - accuracy: 0.9462 - val_loss: 0.3610 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 1s 156ms/step - loss: 0.1908 - accuracy: 0.9401 - val_loss: 0.3584 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.1653 - accuracy: 0.9534 - val_loss: 0.3754 - val_accuracy: 0.9000 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 1s 163ms/step - loss: 0.1599 - accuracy: 0.9534 - val_loss: 0.3792 - val_accuracy: 0.8936 - lr: 0.0010\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 1s 157ms/step - loss: 0.1534 - accuracy: 0.9590 - val_loss: 0.3915 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 1s 159ms/step - loss: 0.1566 - accuracy: 0.9519 - val_loss: 0.3889 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 1s 159ms/step - loss: 0.1598 - accuracy: 0.9549 - val_loss: 0.3581 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 1s 148ms/step - loss: 0.1438 - accuracy: 0.9560 - val_loss: 0.3599 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 1s 156ms/step - loss: 0.1497 - accuracy: 0.9565 - val_loss: 0.3716 - val_accuracy: 0.8957 - lr: 0.0010\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.1262 - accuracy: 0.9657 - val_loss: 0.3680 - val_accuracy: 0.9021 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(validation_split=0.2, rescale=1./255)  # use 20% of data for validation\n",
    "\n",
    "# Load images from directories\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    root_path,\n",
    "    target_size=(40, 60),  # you may need to adjust the target size depending on your image dimensions\n",
    "    color_mode='grayscale',\n",
    "    batch_size=256,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    root_path,\n",
    "    target_size=(40, 60),  # you may need to adjust the target size depending on your image dimensions\n",
    "    color_mode='grayscale',\n",
    "    batch_size=256,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "num_classes = len(train_generator.class_indices)  # get the number of classes\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=(40, 60, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', # which metric to monitor\n",
    "    min_delta=0.001, # minimum change in the monitored metric to be considered as an improvement\n",
    "    patience=10, # number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True, # whether to restore model weights from the epoch with the best value of the monitored metric\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator, \n",
    "    epochs=100, \n",
    "    validation_data=val_generator, \n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "model.save(r'model jadi/model 4.h5')  # saves the model weights and architecture"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
